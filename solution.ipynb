{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler\n",
    "import tqdm\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle\n",
    "\n",
    "from utils import restore_landmarks_batch, restore_landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "NUM_PTS = 194\n",
    "CROP_SIZE = 220\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"./data/test/\" \n",
    "TRAIN_PATH = \"./data/train/\"\n",
    "SUBMISSION_PATH = \"./data/train/\"\n",
    "LANDMARKS = \"./data/train/landmarks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_HEADER = \"file_name,Point_M0_X,Point_M0_Y,Point_M1_X,Point_M1_Y,Point_M2_X,Point_M2_Y,Point_M3_X,Point_M3_Y,Point_M4_X,Point_M4_Y,Point_M5_X,Point_M5_Y,Point_M6_X,Point_M6_Y,Point_M7_X,Point_M7_Y,Point_M8_X,Point_M8_Y,Point_M9_X,Point_M9_Y,Point_M10_X,Point_M10_Y,Point_M11_X,Point_M11_Y,Point_M12_X,Point_M12_Y,Point_M13_X,Point_M13_Y,Point_M14_X,Point_M14_Y,Point_M15_X,Point_M15_Y,Point_M16_X,Point_M16_Y,Point_M17_X,Point_M17_Y,Point_M18_X,Point_M18_Y,Point_M19_X,Point_M19_Y,Point_M20_X,Point_M20_Y,Point_M21_X,Point_M21_Y,Point_M22_X,Point_M22_Y,Point_M23_X,Point_M23_Y,Point_M24_X,Point_M24_Y,Point_M25_X,Point_M25_Y,Point_M26_X,Point_M26_Y,Point_M27_X,Point_M27_Y,Point_M28_X,Point_M28_Y,Point_M29_X,Point_M29_Y\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "10.0\n",
      "7401\n",
      "True\n",
      "1\n",
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.max_memory_allocated(device='cuda'))\n",
    "print(torch.cuda.empty_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleMinSideToSize(object):\n",
    "    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w, _ = sample[self.elem_name].shape\n",
    "        if h > w:\n",
    "            f = self.size[0] / w\n",
    "        else:\n",
    "            f = self.size[1] / h\n",
    "\n",
    "        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n",
    "        sample[\"scale_coef\"] = f\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2).float()\n",
    "            landmarks = landmarks * f\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CropCenter(object):\n",
    "    def __init__(self, size=128, elem_name='image'):\n",
    "        self.size = size\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample[self.elem_name]\n",
    "        h, w, _ = img.shape\n",
    "        margin_h = (h - self.size) // 2\n",
    "        margin_w = (w - self.size) // 2\n",
    "        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
    "        sample[\"crop_margin_x\"] = margin_w\n",
    "        sample[\"crop_margin_y\"] = margin_h\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class TransformByKeys(object):\n",
    "    def __init__(self, transform, names):\n",
    "        self.transform = transform\n",
    "        self.names = set(names)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for name in self.names:\n",
    "            if name in sample:\n",
    "                sample[name] = self.transform(sample[name])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(data.Dataset):\n",
    "    def __init__(self, root, transforms, split=\"train\"):\n",
    "        super(LandmarksDataset, self).__init__()\n",
    "        self.root = root\n",
    "        landmark_file_name = os.path.join(root, 'landmarks.csv') if split is not \"test\" \\\n",
    "            else os.path.join(root, \"test_points.csv\")\n",
    "        images_root = os.path.join(root, \"images\")\n",
    "\n",
    "        self.image_names = []\n",
    "        self.landmarks = []\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            num_lines = sum(1 for line in fp)\n",
    "        num_lines -= 1  # header\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            for i, line in tqdm.tqdm(enumerate(fp)):\n",
    "                if i == 0:\n",
    "                    continue  # skip header\n",
    "                if split == \"train\" and i == int(TRAIN_SIZE * num_lines):\n",
    "                    break  # reached end of train part of data\n",
    "                elif split == \"val\" and i < int(TRAIN_SIZE * num_lines):\n",
    "                    continue  # has not reached start of val part of data\n",
    "                elements = line.strip().split(\",\")\n",
    "                image_name = os.path.join(images_root, elements[0])\n",
    "                self.image_names.append(image_name)\n",
    "\n",
    "                if split in (\"train\", \"val\"):\n",
    "                    landmarks = list(map(np.int16, elements[1:]))\n",
    "                    landmarks = np.array(landmarks, dtype=np.int16).reshape((len(landmarks) // 2, 2))\n",
    "                    self.landmarks.append(landmarks)\n",
    "\n",
    "        if split in (\"train\", \"val\"):\n",
    "            self.landmarks = torch.as_tensor(self.landmarks)\n",
    "        else:\n",
    "            self.landmarks = None\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        if self.landmarks is not None:\n",
    "            landmarks = self.landmarks[idx]\n",
    "            sample[\"landmarks\"] = landmarks\n",
    "\n",
    "        image = cv2.imread(self.image_names[idx])\n",
    "        \n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        sample[\"image\"] = image\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, loader, loss_fn, optimizer,  device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    \n",
    "        \n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"training...\"):\n",
    "        images = batch[\"image\"].to(device)  # B x 3 x CROP_SIZE x CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"]  # B x (2 * NUM_PTS)\n",
    "\n",
    "        pred_landmarks = model(images).cpu()  # B x (2 * NUM_PTS)\n",
    "        loss = loss_fn(pred_landmarks, landmarks, reduction=\"mean\")\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(train_loss, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for batch in tqdm.tqdm(loader, total=len(loader), desc=\"validation...\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks, reduction=\"mean\")\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    for i, batch in enumerate(tqdm.tqdm(loader, total=len(loader), desc=\"test prediction...\")):\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "            pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2))  # B x NUM_PTS x 2\n",
    "\n",
    "        fs = batch[\"scale_coef\"].numpy()  # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy()  # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy()  # B\n",
    "        prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y)  # B x NUM_PTS x 2\n",
    "        predictions[i * loader.batch_size: (i + 1) * loader.batch_size] = prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def create_submission(path_to_data, test_predictions, path_to_submission_file):\n",
    "    test_dir = os.path.join(path_to_data)\n",
    "\n",
    "    output_file = path_to_submission_file\n",
    "    wf = open(output_file, 'w')\n",
    "    wf.write(SUBMISSION_HEADER)\n",
    "\n",
    "    mapping_path = os.path.join(test_dir, 'test_points.csv')\n",
    "    mapping = pd.read_csv(mapping_path, delimiter=',')\n",
    "    \n",
    "    for i, row in mapping.iterrows():\n",
    "        \n",
    "        \n",
    "        file_name = row[0]\n",
    "\n",
    "        point_index_list = np.array(eval(row[1]))\n",
    "        points_for_image = test_predictions[i]\n",
    "        needed_points = points_for_image[point_index_list].astype(np.int)\n",
    "        wf.write(file_name + ',' + ','.join(map(str, needed_points.reshape(2 * len(point_index_list)))) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "        ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "        CropCenter(CROP_SIZE),\n",
    "        TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "        TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "        TransformByKeys(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), (\"image\",)),\n",
    "    ])\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "        ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "        CropCenter(CROP_SIZE),\n",
    "        TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "        TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "        TransformByKeys(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), (\"image\",)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "968it [00:00, 3342.78it/s]\n",
      "2001it [00:00, 14857.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LandmarksDataset(os.path.join(TRAIN_PATH), train_transforms, split=\"train\")\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True,\n",
    "                                   shuffle=True, drop_last=True)\n",
    "val_dataset = LandmarksDataset(os.path.join(TRAIN_PATH), train_transforms, split=\"val\")\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True,\n",
    "                                 shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TResNet(\n",
       "  (body): Sequential(\n",
       "    (SpaceToDepth): SpaceToDepthModule()\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(64, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "          )\n",
       "          (1): AntiAliasDownsampleLayer(\n",
       "            (op): Downsample()\n",
       "          )\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "          )\n",
       "        )\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "          )\n",
       "          (1): AntiAliasDownsampleLayer(\n",
       "            (op): Downsample()\n",
       "          )\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "          )\n",
       "        )\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (se): SEModule(\n",
       "          (avg_pool): FastGlobalAvgPool2d()\n",
       "          (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace)\n",
       "          (fc2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(512, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): InPlaceABN(512, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "          )\n",
       "          (1): AntiAliasDownsampleLayer(\n",
       "            (op): Downsample()\n",
       "          )\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(2048, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): InPlaceABN(2048, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(512, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(512, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(2048, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(512, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(512, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.001])\n",
       "        )\n",
       "        (conv3): Sequential(\n",
       "          (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): InPlaceABN(2048, eps=1e-05, momentum=0.1, affine=True, activation=identity)\n",
       "        )\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_pool): Sequential(\n",
       "    (global_pool_layer): FastGlobalAvgPool2d()\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (fc): Linear(in_features=2048, out_features=388, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = {'num_classes': 2 * NUM_PTS, 'remove_aa_jit': True}\n",
    "model = TResnetM(model_params)\n",
    "\n",
    "# model.head = nn.Linear(model.head.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50(pretrained=True)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-6\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR, amsgrad=True)\n",
    "loss_fn = fnn.mse_loss\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"model_TResnetM.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████| 349/349 [02:56<00:00,  1.98it/s]\n",
      "validation...: 100%|█████████████████████████████████████████████████████████████████| 151/151 [00:34<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0:\ttrain loss:   192.6374\tval loss:   207.2069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████| 349/349 [03:21<00:00,  1.73it/s]\n",
      "validation...: 100%|█████████████████████████████████████████████████████████████████| 151/151 [00:40<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1:\ttrain loss:    191.035\tval loss:   208.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|███████████████████████████████████████████████████████████████████| 349/349 [03:11<00:00,  1.83it/s]\n",
      "validation...: 100%|█████████████████████████████████████████████████████████████████| 151/151 [00:34<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2:\ttrain loss:   199.9937\tval loss:   210.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   3%|█▉                                                                  | 10/349 [00:05<03:13,  1.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cc7a0b16d4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8f04da12dcef>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_loss = train(model, train_dataloader, loss_fn, optimizer,  device=device)\n",
    "    val_loss = validate(model, val_dataloader, loss_fn, device=device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:10.7}\\tval loss: {:10.7}\".format(epoch, train_loss, val_loss))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"model_TResnetM.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331it [00:00, 232548.51it/s]\n",
      "test prediction...: 100%|██████████████████████████████████████████████████████████████| 83/83 [00:29<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = LandmarksDataset(os.path.join(TEST_PATH), test_transforms, split=\"test\")\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)\n",
    "\n",
    "with open(f\"model_TResnetM.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 317.33084106,  377.72793579],\n",
       "        [ 318.58166504,  389.16964722],\n",
       "        [ 320.07702637,  401.0145874 ],\n",
       "        ...,\n",
       "        [ 398.31900024,  300.82006836],\n",
       "        [ 407.21337891,  302.56167603],\n",
       "        [ 416.09152222,  303.44934082]],\n",
       "\n",
       "       [[ 119.62272644,  550.20263672],\n",
       "        [ 122.13540649,  572.86401367],\n",
       "        [ 125.63465118,  596.73095703],\n",
       "        ...,\n",
       "        [ 278.41061401,  406.06585693],\n",
       "        [ 297.35494995,  409.28894043],\n",
       "        [ 316.40991211,  410.71246338]],\n",
       "\n",
       "       [[ 675.46734619, 1614.37451172],\n",
       "        [ 687.48651123, 1704.47644043],\n",
       "        [ 703.05114746, 1798.50354004],\n",
       "        ...,\n",
       "        [1315.46862793, 1026.16235352],\n",
       "        [1393.34069824, 1039.73693848],\n",
       "        [1471.80749512, 1045.3236084 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 570.67840576,  662.34161377],\n",
       "        [ 573.25708008,  688.47094727],\n",
       "        [ 576.52111816,  715.85992432],\n",
       "        ...,\n",
       "        [ 752.57806396,  494.93725586],\n",
       "        [ 773.01324463,  498.59661865],\n",
       "        [ 793.14459229,  500.77331543]],\n",
       "\n",
       "       [[ 139.61190796,  347.82019043],\n",
       "        [ 141.16221619,  362.02111816],\n",
       "        [ 143.23902893,  376.67971802],\n",
       "        ...,\n",
       "        [ 237.93780518,  257.77081299],\n",
       "        [ 249.41687012,  259.75579834],\n",
       "        [ 260.92431641,  260.84344482]],\n",
       "\n",
       "       [[ 738.70288086, 1010.3927002 ],\n",
       "        [ 741.15112305, 1036.37268066],\n",
       "        [ 744.5189209 , 1063.38269043],\n",
       "        ...,\n",
       "        [ 924.42327881,  828.02435303],\n",
       "        [ 944.60150146,  832.58520508],\n",
       "        [ 965.23034668,  835.40319824]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                 \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission(TEST_PATH, test_predictions, f\"submit_TResnetM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
